# sqlite3perf

This repository is cloned from [mwmahlberg/sqlite3perf](https://github.com/mwmahlberg/sqlite3perf).

This repository contains a small application which was created while researching a proper 
answer to the question [Faster sqlite 3 query in go? I need to process 1million+ rows as fast as possible][so:oq].

The assumption there was that Python is faster with accessing SQLite3 than Go is.

I wanted to check this and hence I wrote a generator for entries into an SQLite database as well as a Go implementation and a Python implementation of a simple access task:

1. Read all rows from table bench, which consists of an ID, a hex encoded 8 byte random value and a hex encoded SHA256 hash of said random values.
2. Create a SHA256 hex encoded checksum from the decoded random value of a row.
3. Compare the stored hash value against the generated one.
4. If they match, continue, otherwise throw an error.

[so:oq]: https://stackoverflow.com/questions/48000940/

## Introduction

My assumption was that we have a problem with how the performance is measured here, so I wrote a little Go program to generate records and save them into a SQLite database as well as a Python and Go implementation of a little task to do on those records.

You can find the according repository at [mwmahlberg/sqlite3perf](https://github.com/mwmahlberg/sqlite3perf)

### The data model

The records generated consist of

- ID: [A row ID generated by SQLite](https://sqlite.org/lang_createtable.html#rowid)
- rand: A hex encoded, 8 byte, pseudo-random value
- hash: A hex encoded, SHA256 hash of the unencoded rand

The table's schema is relatively simple:

```sql
$ sqlite3 sqlite3perf.db
SQLite version 3.28.0 2019-04-15 14:49:49
Enter ".help" for usage hints.
sqlite> .schema
CREATE TABLE bench (ID int PRIMARY KEY ASC, rand TEXT, hash TEXT);
```

First I generated 1.5M records and vacuumed the sqlite database afterwards with

```bash
$ sqlite3perf generate -r 1500000 -v
2020/07/09 22:34:44 Generating 1500000 records
2020/07/09 22:34:44 Opening database
2020/07/09 22:34:44 Dropping table 'bench' if already present
2020/07/09 22:34:44 (Re-)creating table 'bench'
2020/07/09 22:34:44 Setting up the environment
2020/07/09 22:34:44 Starting progress logging
2020/07/09 22:34:44 Starting inserts
2020/07/09 22:34:46  240099/1500000 ( 16.01%) written in 2.000146054s, avg: 8.33µs/record, 120040.73 records/s
2020/07/09 22:34:48  488499/1500000 ( 32.57%) written in 4.000127495s, avg: 8.188µs/record, 122120.86 records/s
2020/07/09 22:34:50  730799/1500000 ( 48.72%) written in 6.000064551s, avg: 8.21µs/record, 121798.52 records/s
2020/07/09 22:34:52  968999/1500000 ( 64.60%) written in 8.000359735s, avg: 8.256µs/record, 121119.43 records/s
2020/07/09 22:34:54 1200899/1500000 ( 80.06%) written in 10.000176936s, avg: 8.327µs/record, 120087.78 records/s
2020/07/09 22:34:56 1430799/1500000 ( 95.39%) written in 12.000074408s, avg: 8.386µs/record, 119232.51 records/s
2020/07/09 22:34:56 1500000/1500000 (100.00%) written in 12.537909385s, avg: 8.358µs/record, 119637.17 records/s
2020/07/09 22:34:56 Vaccumating database file
2020/07/09 22:34:58 Vacuumation took 2.070888698s
```

Next I called the Go implementation against those 1.5M records. Both the Go as well as the Python implementation 
basically do the same simple task:

1. Read all entries from the database.
1. For each row, decode the random value from hex, then create a SHA256 hex from the result.
1. Compare the generated SHA256 hex string against the one stored in the database
1. If they match, continue, otherwise break.

### Assumptions

My assumption explicitly was that Python did some type of lazy loading and/or possibly even execution of the SQL query.

## The results

### Go implementation

```bash
$ sqlite3perf bench 
2020/07/09 22:37:23 Running benchmark
2020/07/09 22:37:23 Time after query: 1.261861ms
2020/07/09 22:37:23 Beginning loop
2020/07/09 22:37:23 Acessing the first result set 
        ID 0,
        rand: 819f4b54a911924d,
        hash: 507d24d4ae8ec1b7c89939abc6c80959ce7f04334c6d9c3b15ac86c7aaef24da
took 123.618µs
2020/07/09 22:37:25 1,101,829 rows processed
2020/07/09 22:37:26 1,500,000 rows processed
2020/07/09 22:37:26 Finished loop after 2.71359178s
2020/07/09 22:37:26 Average 1.809µs per record, 2.714910396s overall
```

Note the values for "time after query" ( the time the query command took to return) 
and the time it took to access the first result set after the iteration over the result set was started.

### Python implementation

```bash
$ python bench.py 
07/09/2020 22:38:19 Starting up
07/09/2020 22:38:19 Time after query: 232µs
07/09/2020 22:38:19 Beginning loop
07/09/2020 22:38:20 Accessing first result set
        ID: 0
        rand: 819f4b54a911924d
        hash: 507d24d4ae8ec1b7c89939abc6c80959ce7f04334c6d9c3b15ac86c7aaef24da
took 1.544171 s
07/09/2020 22:38:24 Finished loop after 5.655742s
07/09/2020 22:38:24 Average: 3.770µs per record, 0:00:05.656060 overall
```

Again, note the value for "time after query" and the time it took to access the first result set.

## Summary

It took the Go implementation quite a while to return after the SELECT query was send, while Python seemed to be blazing fast in comparison. However, from the time it took to actually access the first result set, we can see that the Go implementation is more than 500 times faster to actually access the first result set (5.372329ms vs 2719.312ms) and about double as fast for the task at hand as the Python implementation.

## Notes

- In order to prove the assumption that Python actually does lazy loading on the result set, each and every row and column had to be accessed in order to make sure that Python is forced to actually read the value from the database.
- I chose a hashing task because presumably the implementation of SHA256 is highly optimised in both languages.

## Conclusion

Python does seem to do lazy loading of result sets and possibly does not even execute a query unless the according result set 
is actually accessed. In this simulated scenario, mattn's SQLite driver for Go outperforms Python's by between roughly 100% 
and orders of magnitude, depending on what you want to do.

Edit: So in order to have a fast processing, implement your task in Go. While it takes longer to send the actual query, 
accessing the individual rows of the result set is by far faster. I'd suggest starting out with a small subset of your data, 
say 50k records. Then, to further improve your code, use [profiling](https://blog.golang.org/profiling-go-programs) 
to identify your bottlenecks. 

Depending on what you want to do during processing, [pipelines](https://blog.golang.org/pipelines) for example might help, 
but how to improve the processing 
speed of the task at hand is difficult to say without actual code or a thorough description.